---
title: "Eval_Lab"
author: "Brian Wright"
date: "10/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(caret)
library(class)
library(dplyr)
#install.packages("MLmetrics")
library(MLmetrics)
#install.packages("ROCR")
library(ROCR)
```

Throughout your early career as a data scientist you've built complex visualizations, explored NBA talent, minded text on Data Science news and gained a better understanding how to create commercials with great success but you've suddenly realized you need to enhance your ability to assess the models you are building. As the most important part about understanding any machine learning model (any model) is understanding it's weakness or better said it's vulnerabilities. 

In doing so you've decided to practice on datasets that are of interest to you, but use a 
approach to which you are very familiar, kNN. 

Part 1. Select either as a lab or individual two datasets that you have not used before but that are of interest to you/group. Define questions that can be answered using a classification, specifically kNN, for each dataset. Build kNN models and then use the evaluation metrics we discussed in class (Accuracy, TPR, FPR, F1, Kappa, LogLoss and ROC/AUC) to assess the quality of the models. Make sure to calculate the base rate or prevalence to provide a reference for some of these measures.

Part 2. Take a closer look at where miss-classification errors are occurring, is there a pattern? If so discuss this pattern and why you think this is the case. 

Part 3. Based on your exploration in Part 2, change the threshold using the function provided, what differences do you see in the evaluation metrics? Speak specifically to the metrics you think are best suited to address the questions you are trying to answer. 

Part 4. Summarize your findings to include recommendations on how you might change each of the two kNN models based on the results. These recommendations might include gathering more data, adjusting the threshold or maybe that it's working fine at the current level and nothing should be done. Regardless of the outcome, what should we be aware of when these models are deployed? 

Weather Dataset

```{r}

set.seed(2001)

weatherAUS_data <- read.csv("weatherAUS.csv", #<- name of the data set.
                     check.names = FALSE, #<- don't change column names.
                     stringsAsFactors = FALSE)#<- don't convert the numbers and characters to factors

weatherAUS_data.cleaned <- weatherAUS_data
weatherAUS_data.cleaned <- weatherAUS_data.cleaned[,c(-1,-2)]
weatherAUS_data.cleaned <- weatherAUS_data.cleaned[, which(colMeans(is.na(weatherAUS_data.cleaned)) < 0.3)]
weatherAUS_data.cleaned <- weatherAUS_data.cleaned[complete.cases(weatherAUS_data.cleaned), ]
weatherAUS_data.cleaned <- weatherAUS_data.cleaned %>% select(-contains("Dir"))

weatherAUS_data.cleaned$RainToday <- recode(weatherAUS_data.cleaned$RainToday, 'No' = 0, 'Yes' = 1)
weatherAUS_data.cleaned$RainTomorrow <- recode(weatherAUS_data.cleaned$RainTomorrow, 'No' = 0, 'Yes' = 1)

correlations <- cor(weatherAUS_data.cleaned)

weatherAUS_data.cleaned[, -ncol(weatherAUS_data.cleaned)] <- lapply(weatherAUS_data.cleaned[, -ncol(weatherAUS_data.cleaned)],function(x) scale(x))

weatherAUS_data.shortened <- weatherAUS_data.cleaned[1:10000,]


#caret function the will allow us to divide the data into test and train, it will randomly assign rows into each category while maintaining the relative balance (0 and 1s) of the target variable. 
split_index <- createDataPartition(weatherAUS_data.cleaned$RainTomorrow, p = .8, # split 80% - 20% 
                                  list = FALSE,#output of the data, we don't want a list
                                  times = 1)#the number of partitions to create we just want one

split_index_shortened <- createDataPartition(weatherAUS_data.shortened$RainTomorrow, p = .8, # split 80% - 20% 
                                  list = FALSE,#output of the data, we don't want a list
                                  times = 1)#the number of partitions to create we just want one


#then we just pass the index to our dataset

weatherAUS_data_train <- weatherAUS_data.cleaned[split_index,]
dim(weatherAUS_data_train)

weatherAUS_data_test <- weatherAUS_data.cleaned[-split_index,]
dim(weatherAUS_data_test)


weatherAUS_data_train.shortened <- weatherAUS_data.shortened[split_index_shortened,]
weatherAUS_data_test.shortened <- weatherAUS_data.shortened[-split_index_shortened,]



```
```{r}
trctrl <- trainControl(method = "repeatedcv",
                       number = 10,
                       repeats = 3) # generic control to pass back into the knn mode using the cross validation method. 
weatherAUS_data_train.shortened$RainTomorrow <- as.factor(weatherAUS_data_train.shortened$RainTomorrow)

weatherAUS_knn_caret <- train(RainTomorrow~.,
                  data = weatherAUS_data_train.shortened,
                  method="knn",
                  tuneLength=5,
                  trControl= trctrl,#cv method above, will select the optimal K
                  preProcess="scale") # already did this but helpful reference
weatherAUS_knn_caret # take a look

```
```{r}

# KNN Work

weatherAUS_13NN <-  knn(train = weatherAUS_data_train[,-ncol(weatherAUS_data_train)],#<- training set cases
               test = weatherAUS_data_test[,-ncol(weatherAUS_data_test)],    #<- test set cases
               cl = weatherAUS_data_train[,"RainTomorrow"],#<- category for true classification
               k = 13,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE) #<- control ties between class assignments If true, all distances equal to the kth largest are included
length(weatherAUS_13NN)

confusionMatrix(as.factor(weatherAUS_13NN), as.factor(weatherAUS_data_test$RainTomorrow), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

```
Here we see that the Accuracy is 84.52%, which is pretty good. The Kappa value is 47.39%

To get the TPR and FPR, we used the following formulas. 

* tpr = tp / (tp + fn)
* fpr = fp / (fp + tn)


```{r}
tpr <- 2356/(2356+2615)
sprintf("TPR Calculated: %s", tpr)
```
```{r}
fpr <- 873/(873 + 16690)
sprintf("FPR Calculated: %s", fpr)
```

Next, we calculated the F1 Score
```{r}
F1 <- F1_Score(y_pred = weatherAUS_13NN, y_true = weatherAUS_data_test$RainTomorrow, positive = "1")
sprintf("F1 of Model: %s", F1)
```

We then found the LogLoss
```{r}
#In order to use most evaluation packages it's just easier to have are predictions and targets in one place. 

prob_knn <- tibble(weatherAUS_13NN, attributes(weatherAUS_13NN)$prob)
prob_knn$prob <- if_else(prob_knn$weatherAUS_13NN == 0,
                         1-prob_knn$`attributes(weatherAUS_13NN)$prob`, prob_knn$`attributes(weatherAUS_13NN)$prob`) #### this is a example of converting the probabilities to the correct format.
weatherAUS_eval1 <- data.frame(pred_class = weatherAUS_13NN, 
                        pred_prob = prob_knn$prob, 
                        target = as.numeric(weatherAUS_data_test$RainTomorrow))

weatherAUS_pred <- prediction(weatherAUS_eval$pred_prob, weatherAUS_eval$target)
weatherAUS_roc <- performance(weatherAUS_pred, "tpr", "fpr")

ll <- LogLoss(as.numeric(weatherAUS_eval$pred_prob), as.numeric(weatherAUS_data_test$RainTomorrow))
sprintf("LogLoss of Model: %s", ll)
```
We then plotted the ROC AUC curve.
```{r}
plot(weatherAUS_roc, colorize = TRUE)+abline(a=0, b= 1)
```
```{r}
pred <- prediction(as.numeric(weatherAUS_eval$pred_prob), as.numeric(weatherAUS_data_test$RainTomorrow))

knn_perf <- performance(pred,"tpr","fpr")

KNN_perf_AUC <- performance(pred,"auc")

paste("AUC: ", KNN_perf_AUC@y.values)
```
Based on the ROC curve, we chose a threshold value of 0.40. 
```{r}
adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=test_outcome
  thres <- as.factor(ifelse(x > y, 1,0))
  confusionMatrix(thres, z, positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")
}

adjust_thres(weatherAUS_eval$pred_prob, .40, as.factor(weatherAUS_data_test$RainTomorrow))
```




